{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # obtains tokens with a least 1 alphabet\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())\n",
    "\n",
    "def mapping(tokens):\n",
    "    word_to_id = dict()\n",
    "    id_to_word = dict()\n",
    "\n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "def generate_training_data(tokens, word_to_id, window_size):\n",
    "    N = len(tokens)\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(N):\n",
    "        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n",
    "                   list(range(i + 1, min(N, i + window_size + 1)))\n",
    "        for j in nbr_inds:\n",
    "            X.append(word_to_id[tokens[i]])\n",
    "            Y.append(word_to_id[tokens[j]])\n",
    "            \n",
    "    X = np.array(X)\n",
    "    X = np.expand_dims(X, axis=0)\n",
    "    Y = np.array(Y)\n",
    "    Y = np.expand_dims(Y, axis=0)\n",
    "            \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"After the deduction of the costs of investing, \" \\\n",
    "      \"beating the stock market is a loser's game.\"\n",
    "tokens = tokenize(doc)\n",
    "word_to_id, id_to_word = mapping(tokens)\n",
    "X, Y = generate_training_data(tokens, word_to_id, 3)\n",
    "vocab_size = len(id_to_word)\n",
    "m = Y.shape[1]\n",
    "# turn Y into one hot encoding\n",
    "Y_one_hot = np.zeros((vocab_size, m))\n",
    "Y_one_hot[Y.flatten(), np.arange(m)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_wrd_emb(vocab_size, emb_size):\n",
    "    \"\"\"\n",
    "    vocab_size: int. vocabulary size of your corpus or training data\n",
    "    emb_size: int. word embedding size. How many dimensions to represent each vocabulary\n",
    "    \"\"\"\n",
    "    WRD_EMB = np.random.randn(vocab_size, emb_size) * 0.01\n",
    "    \n",
    "    assert(WRD_EMB.shape == (vocab_size, emb_size))\n",
    "    return WRD_EMB\n",
    "\n",
    "def initialize_dense(input_size, output_size):\n",
    "    \"\"\"\n",
    "    input_size: int. size of the input to the dense layer\n",
    "    output_szie: int. size of the output out of the dense layer\n",
    "    \"\"\"\n",
    "    W = np.random.randn(output_size, input_size) * 0.01\n",
    "    \n",
    "    assert(W.shape == (output_size, input_size))\n",
    "    return W\n",
    "\n",
    "def initialize_parameters(vocab_size, emb_size):\n",
    "    WRD_EMB = initialize_wrd_emb(vocab_size, emb_size)\n",
    "    W = initialize_dense(emb_size, vocab_size)\n",
    "    \n",
    "    parameters = {}\n",
    "    parameters['WRD_EMB'] = WRD_EMB\n",
    "    parameters['W'] = W\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_to_word_vecs(inds, parameters):\n",
    "    \"\"\"\n",
    "    inds: numpy array. shape: (1, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = inds.shape[1]\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    word_vec = WRD_EMB[inds.flatten(), :].T\n",
    "    \n",
    "    assert(word_vec.shape == (WRD_EMB.shape[1], m))\n",
    "    \n",
    "    return word_vec\n",
    "\n",
    "def linear_dense(word_vec, parameters):\n",
    "    \"\"\"\n",
    "    word_vec: numpy array. shape: (emb_size, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = word_vec.shape[1]\n",
    "    W = parameters['W']\n",
    "    Z = np.dot(W, word_vec)\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], m))\n",
    "    \n",
    "    return W, Z\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Z: output out of the dense layer. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    softmax_out = np.divide(np.exp(Z), np.sum(np.exp(Z), axis=0, keepdims=True) + 0.001)\n",
    "    assert(softmax_out.shape == Z.shape)\n",
    "\n",
    "    return softmax_out\n",
    "\n",
    "def forward_propagation(inds, parameters):\n",
    "    word_vec = ind_to_word_vecs(inds, parameters)\n",
    "    W, Z = linear_dense(word_vec, parameters)\n",
    "    softmax_out = softmax(Z)\n",
    "    \n",
    "    caches = {}\n",
    "    caches['inds'] = inds\n",
    "    caches['word_vec'] = word_vec\n",
    "    caches['W'] = W\n",
    "    caches['Z'] = Z\n",
    "    return softmax_out, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(softmax_out, Y):\n",
    "    \"\"\"\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    m = softmax_out.shape[1]\n",
    "    cost = -(1 / m) * np.sum(np.log(softmax_out[Y.flatten(), np.arange(Y.shape[1])] + 0.001))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(Y, softmax_out):\n",
    "    \"\"\"\n",
    "    Y: labels of training data. shape: (vocab_size, m)\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    softmax_out[Y.flatten(), np.arange(m)] -= 1.0\n",
    "    dL_dZ = softmax_out\n",
    "    \n",
    "    assert(dL_dZ.shape == softmax_out.shape)\n",
    "    return dL_dZ\n",
    "\n",
    "def dense_backward(dL_dZ, caches):\n",
    "    \"\"\"\n",
    "    dL_dZ: shape: (vocab_size, m)\n",
    "    caches: dict. results from each steps of forward propagation\n",
    "    \"\"\"\n",
    "    W = caches['W']\n",
    "    word_vec = caches['word_vec']\n",
    "    m = word_vec.shape[1]\n",
    "    \n",
    "    dL_dW = (1 / m) * np.dot(dL_dZ, word_vec.T)\n",
    "    dL_dword_vec = np.dot(W.T, dL_dZ)\n",
    "\n",
    "    assert(W.shape == dL_dW.shape)\n",
    "    assert(word_vec.shape == dL_dword_vec.shape)\n",
    "    \n",
    "    return dL_dW, dL_dword_vec\n",
    "\n",
    "def backward_propagation(Y, softmax_out, caches):\n",
    "    dL_dZ = softmax_backward(Y, softmax_out)\n",
    "    dL_dW, dL_dword_vec = dense_backward(dL_dZ, caches)\n",
    "    \n",
    "    gradients = dict()\n",
    "    gradients['dL_dZ'] = dL_dZ\n",
    "    gradients['dL_dW'] = dL_dW\n",
    "    gradients['dL_dword_vec'] = dL_dword_vec\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, caches, gradients, learning_rate):\n",
    "    vocab_size, emb_size = parameters['WRD_EMB'].shape\n",
    "    inds = caches['inds']\n",
    "    dL_dword_vec = gradients['dL_dword_vec']\n",
    "    m = inds.shape[-1]\n",
    "    \n",
    "    parameters['WRD_EMB'][inds.flatten(), :] -= dL_dword_vec.T * learning_rate\n",
    "\n",
    "    parameters['W'] -= learning_rate * gradients['dL_dW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def skipgram_model_training(X, Y, vocab_size, emb_size, learning_rate, epochs, batch_size=256, parameters=None, print_cost=False, plot_cost=True):\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    if parameters is None:\n",
    "        parameters = initialize_parameters(vocab_size, emb_size)\n",
    "    \n",
    "    begin_time = datetime.now()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_cost = 0\n",
    "        batch_inds = list(range(0, m, batch_size))\n",
    "        np.random.shuffle(batch_inds)\n",
    "        for i in batch_inds:\n",
    "            X_batch = X[:, i:i+batch_size]\n",
    "            Y_batch = Y[:, i:i+batch_size]\n",
    "\n",
    "            softmax_out, caches = forward_propagation(X_batch, parameters)\n",
    "            cost = cross_entropy(softmax_out, Y_batch)\n",
    "            gradients = backward_propagation(Y_batch, softmax_out, caches)\n",
    "            update_parameters(parameters, caches, gradients, learning_rate)\n",
    "            epoch_cost += np.squeeze(cost)\n",
    "            \n",
    "        costs.append(epoch_cost)\n",
    "        if print_cost and epoch % (epochs // 500) == 0:\n",
    "            print(\"Cost after epoch {}: {}\".format(epoch, epoch_cost))\n",
    "        if epoch % (epochs // 100) == 0:\n",
    "            learning_rate *= 0.98\n",
    "    end_time = datetime.now()\n",
    "    print('training time: {}'.format(end_time - begin_time))\n",
    "            \n",
    "    if plot_cost:\n",
    "        plt.plot(np.arange(epochs), costs)\n",
    "        plt.xlabel('# of epochs')\n",
    "        plt.ylabel('cost')\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 2.552139316550904\n",
      "Cost after epoch 10: 2.551816018233629\n",
      "Cost after epoch 20: 2.551475689269118\n",
      "Cost after epoch 30: 2.551097272367718\n",
      "Cost after epoch 40: 2.5506593132658444\n",
      "Cost after epoch 50: 2.5501386425953254\n",
      "Cost after epoch 60: 2.5495220769895197\n",
      "Cost after epoch 70: 2.5487755651836594\n",
      "Cost after epoch 80: 2.5478673231831985\n",
      "Cost after epoch 90: 2.5467623062184246\n",
      "Cost after epoch 100: 2.545420259087787\n",
      "Cost after epoch 110: 2.543826823824382\n",
      "Cost after epoch 120: 2.541914104520238\n",
      "Cost after epoch 130: 2.5396221491947233\n",
      "Cost after epoch 140: 2.5368864252409664\n",
      "Cost after epoch 150: 2.5336346720561918\n",
      "Cost after epoch 160: 2.52986195840055\n",
      "Cost after epoch 170: 2.5254429597030263\n",
      "Cost after epoch 180: 2.520285720342923\n",
      "Cost after epoch 190: 2.5143047360964066\n",
      "Cost after epoch 200: 2.507418991058852\n",
      "Cost after epoch 210: 2.4997091921577086\n",
      "Cost after epoch 220: 2.4910328768933487\n",
      "Cost after epoch 230: 2.4813601805276706\n",
      "Cost after epoch 240: 2.4707181400786906\n",
      "Cost after epoch 250: 2.4591878319760285\n",
      "Cost after epoch 260: 2.4471398747022595\n",
      "Cost after epoch 270: 2.4346002924892916\n",
      "Cost after epoch 280: 2.421800961685695\n",
      "Cost after epoch 290: 2.4090391593321514\n",
      "Cost after epoch 300: 2.396627292231331\n",
      "Cost after epoch 310: 2.385063309196559\n",
      "Cost after epoch 320: 2.374365746467409\n",
      "Cost after epoch 330: 2.3646558085070524\n",
      "Cost after epoch 340: 2.3559865848816957\n",
      "Cost after epoch 350: 2.3483205132789307\n",
      "Cost after epoch 360: 2.3416603195607903\n",
      "Cost after epoch 370: 2.3357155698652288\n",
      "Cost after epoch 380: 2.330293213369492\n",
      "Cost after epoch 390: 2.325209347419569\n",
      "Cost after epoch 400: 2.320288795518982\n",
      "Cost after epoch 410: 2.315463280660533\n",
      "Cost after epoch 420: 2.3105253804616144\n",
      "Cost after epoch 430: 2.3053610218105884\n",
      "Cost after epoch 440: 2.299889516920493\n",
      "Cost after epoch 450: 2.2940533663445137\n",
      "Cost after epoch 460: 2.2879329949216203\n",
      "Cost after epoch 470: 2.2814255722815484\n",
      "Cost after epoch 480: 2.2745207309305444\n",
      "Cost after epoch 490: 2.267234193605206\n",
      "Cost after epoch 500: 2.2595933954484626\n",
      "Cost after epoch 510: 2.2517813853463795\n",
      "Cost after epoch 520: 2.2437226959834935\n",
      "Cost after epoch 530: 2.23544983219968\n",
      "Cost after epoch 540: 2.227015594330619\n",
      "Cost after epoch 550: 2.218476177501307\n",
      "Cost after epoch 560: 2.210044697469987\n",
      "Cost after epoch 570: 2.2016403636694313\n",
      "Cost after epoch 580: 2.1932995752038655\n",
      "Cost after epoch 590: 2.1850727802648384\n",
      "Cost after epoch 600: 2.177005719514545\n",
      "Cost after epoch 610: 2.1692782312128887\n",
      "Cost after epoch 620: 2.1617895008418015\n",
      "Cost after epoch 630: 2.154548028490746\n",
      "Cost after epoch 640: 2.147571820000419\n",
      "Cost after epoch 650: 2.140872827671324\n",
      "Cost after epoch 660: 2.134570972038838\n",
      "Cost after epoch 670: 2.128556730520046\n",
      "Cost after epoch 680: 2.1228154156325494\n",
      "Cost after epoch 690: 2.1173428214313006\n",
      "Cost after epoch 700: 2.112132759381189\n",
      "Cost after epoch 710: 2.1072649959227223\n",
      "Cost after epoch 720: 2.102644354872224\n",
      "Cost after epoch 730: 2.0982523024204567\n",
      "Cost after epoch 740: 2.094080381981644\n",
      "Cost after epoch 750: 2.0901202287574705\n",
      "Cost after epoch 760: 2.086429866303228\n",
      "Cost after epoch 770: 2.082935471022718\n",
      "Cost after epoch 780: 2.079622317101322\n",
      "Cost after epoch 790: 2.0764837913995935\n",
      "Cost after epoch 800: 2.073513637092894\n",
      "Cost after epoch 810: 2.0707553049636642\n",
      "Cost after epoch 820: 2.0681534638965187\n",
      "Cost after epoch 830: 2.065697222269266\n",
      "Cost after epoch 840: 2.0633816728735406\n",
      "Cost after epoch 850: 2.0612020480632145\n",
      "Cost after epoch 860: 2.059189632380654\n",
      "Cost after epoch 870: 2.057303192461348\n",
      "Cost after epoch 880: 2.0555341815176793\n",
      "Cost after epoch 890: 2.053878323464248\n",
      "Cost after epoch 900: 2.052331352084915\n",
      "Cost after epoch 910: 2.050914261057058\n",
      "Cost after epoch 920: 2.049596654542791\n",
      "Cost after epoch 930: 2.0483715242958267\n",
      "Cost after epoch 940: 2.047234892874732\n",
      "Cost after epoch 950: 2.0461828165906364\n",
      "Cost after epoch 960: 2.0452283697357805\n",
      "Cost after epoch 970: 2.044349816848866\n",
      "Cost after epoch 980: 2.0435415684952485\n",
      "Cost after epoch 990: 2.0428001590838085\n",
      "Cost after epoch 1000: 2.0421222159523515\n",
      "Cost after epoch 1010: 2.041515237220754\n",
      "Cost after epoch 1020: 2.0409644045062922\n",
      "Cost after epoch 1030: 2.04046552626624\n",
      "Cost after epoch 1040: 2.0400158329299134\n",
      "Cost after epoch 1050: 2.039612665447787\n",
      "Cost after epoch 1060: 2.0392597190605404\n",
      "Cost after epoch 1070: 2.0389474878248186\n",
      "Cost after epoch 1080: 2.0386729860642796\n",
      "Cost after epoch 1090: 2.038434102502619\n",
      "Cost after epoch 1100: 2.038228817703582\n",
      "Cost after epoch 1110: 2.038058200668409\n",
      "Cost after epoch 1120: 2.037916648472492\n",
      "Cost after epoch 1130: 2.037802090912635\n",
      "Cost after epoch 1140: 2.037712921819146\n",
      "Cost after epoch 1150: 2.0376476002053088\n",
      "Cost after epoch 1160: 2.037605367705753\n",
      "Cost after epoch 1170: 2.0375833990435654\n",
      "Cost after epoch 1180: 2.0375802747131813\n",
      "Cost after epoch 1190: 2.037594746889794\n",
      "Cost after epoch 1200: 2.0376256131014174\n",
      "Cost after epoch 1210: 2.0376708867207434\n",
      "Cost after epoch 1220: 2.0377296724891676\n",
      "Cost after epoch 1230: 2.037801008530012\n",
      "Cost after epoch 1240: 2.037883905946025\n",
      "Cost after epoch 1250: 2.0379774094148484\n",
      "Cost after epoch 1260: 2.038078782574678\n",
      "Cost after epoch 1270: 2.0381884287802654\n",
      "Cost after epoch 1280: 2.0383057122669785\n",
      "Cost after epoch 1290: 2.0384298425566136\n",
      "Cost after epoch 1300: 2.038560055929957\n",
      "Cost after epoch 1310: 2.0386932498751387\n",
      "Cost after epoch 1320: 2.0388306499790603\n",
      "Cost after epoch 1330: 2.0389718548030706\n",
      "Cost after epoch 1340: 2.039116234046619\n",
      "Cost after epoch 1350: 2.039263179977748\n",
      "Cost after epoch 1360: 2.039409524677213\n",
      "Cost after epoch 1370: 2.039556953369518\n",
      "Cost after epoch 1380: 2.039705232593051\n",
      "Cost after epoch 1390: 2.039853866638732\n",
      "Cost after epoch 1400: 2.0400023798728704\n",
      "Cost after epoch 1410: 2.0401477684936444\n",
      "Cost after epoch 1420: 2.040291903842316\n",
      "Cost after epoch 1430: 2.04043467278653\n",
      "Cost after epoch 1440: 2.04057569745017\n",
      "Cost after epoch 1450: 2.040714618837109\n",
      "Cost after epoch 1460: 2.040848765770102\n",
      "Cost after epoch 1470: 2.0409799950699923\n",
      "Cost after epoch 1480: 2.0411082808939494\n",
      "Cost after epoch 1490: 2.0412333526702944\n",
      "Cost after epoch 1500: 2.04135495825918\n",
      "Cost after epoch 1510: 2.0414708729633135\n",
      "Cost after epoch 1520: 2.041582798666747\n",
      "Cost after epoch 1530: 2.04169077399185\n",
      "Cost after epoch 1540: 2.0417946278266292\n",
      "Cost after epoch 1550: 2.041894207001496\n",
      "Cost after epoch 1560: 2.04198779568453\n",
      "Cost after epoch 1570: 2.0420768497351007\n",
      "Cost after epoch 1580: 2.0421614548236593\n",
      "Cost after epoch 1590: 2.0422415301284134\n",
      "Cost after epoch 1600: 2.0423170114947204\n",
      "Cost after epoch 1610: 2.042386705879193\n",
      "Cost after epoch 1620: 2.04245177195947\n",
      "Cost after epoch 1630: 2.0425123264922966\n",
      "Cost after epoch 1640: 2.042568365556139\n",
      "Cost after epoch 1650: 2.04261989945734\n",
      "Cost after epoch 1660: 2.042666229298415\n",
      "Cost after epoch 1670: 2.042708197359422\n",
      "Cost after epoch 1680: 2.0427459350479342\n",
      "Cost after epoch 1690: 2.042779497151672\n",
      "Cost after epoch 1700: 2.042808949220705\n",
      "Cost after epoch 1710: 2.042834025408047\n",
      "Cost after epoch 1720: 2.042855257425002\n",
      "Cost after epoch 1730: 2.0428727747897204\n",
      "Cost after epoch 1740: 2.042886669867229\n",
      "Cost after epoch 1750: 2.0428970418621657\n",
      "Cost after epoch 1760: 2.0429039778037494\n",
      "Cost after epoch 1770: 2.0429077234570396\n",
      "Cost after epoch 1780: 2.042908391776017\n",
      "Cost after epoch 1790: 2.0429060919982778\n",
      "Cost after epoch 1800: 2.0429009365616952\n",
      "Cost after epoch 1810: 2.0428932801091553\n",
      "Cost after epoch 1820: 2.0428831199194595\n",
      "Cost after epoch 1830: 2.042870541877206\n",
      "Cost after epoch 1840: 2.042855655212602\n",
      "Cost after epoch 1850: 2.042838569611371\n",
      "Cost after epoch 1860: 2.042819828544308\n",
      "Cost after epoch 1870: 2.0427992253779004\n",
      "Cost after epoch 1880: 2.0427768140258205\n",
      "Cost after epoch 1890: 2.04275269278438\n",
      "Cost after epoch 1900: 2.042726958838463\n",
      "Cost after epoch 1910: 2.042700279224979\n",
      "Cost after epoch 1920: 2.0426722898939844\n",
      "Cost after epoch 1930: 2.04264301346967\n",
      "Cost after epoch 1940: 2.0426125326545335\n",
      "Cost after epoch 1950: 2.0425809285824617\n",
      "Cost after epoch 1960: 2.042548942240057\n",
      "Cost after epoch 1970: 2.0425160964443942\n",
      "Cost after epoch 1980: 2.042482387505467\n",
      "Cost after epoch 1990: 2.0424478829303845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 2000: 2.042412649004743\n",
      "Cost after epoch 2010: 2.042377464966704\n",
      "Cost after epoch 2020: 2.0423417799290196\n",
      "Cost after epoch 2030: 2.0423055712467613\n",
      "Cost after epoch 2040: 2.042268894706715\n",
      "Cost after epoch 2050: 2.0422318056330298\n",
      "Cost after epoch 2060: 2.0421950959027964\n",
      "Cost after epoch 2070: 2.042158174401462\n",
      "Cost after epoch 2080: 2.0421210073812603\n",
      "Cost after epoch 2090: 2.04208364341433\n",
      "Cost after epoch 2100: 2.0420461314127643\n",
      "Cost after epoch 2110: 2.0420092560565966\n",
      "Cost after epoch 2120: 2.0419724136212745\n",
      "Cost after epoch 2130: 2.0419355661049963\n",
      "Cost after epoch 2140: 2.041898758729749\n",
      "Cost after epoch 2150: 2.0418620376230154\n",
      "Cost after epoch 2160: 2.0418261626017693\n",
      "Cost after epoch 2170: 2.0417905398289955\n",
      "Cost after epoch 2180: 2.041755131946994\n",
      "Cost after epoch 2190: 2.0417199829742088\n",
      "Cost after epoch 2200: 2.041685138005994\n",
      "Cost after epoch 2210: 2.0416513143131585\n",
      "Cost after epoch 2220: 2.041617946243039\n",
      "Cost after epoch 2230: 2.0415849996856656\n",
      "Cost after epoch 2240: 2.041552517506252\n",
      "Cost after epoch 2250: 2.041520543406829\n",
      "Cost after epoch 2260: 2.0414897339207263\n",
      "Cost after epoch 2270: 2.0414595664765742\n",
      "Cost after epoch 2280: 2.0414300107560295\n",
      "Cost after epoch 2290: 2.041401106721363\n",
      "Cost after epoch 2300: 2.0413728946250753\n",
      "Cost after epoch 2310: 2.0413459525734394\n",
      "Cost after epoch 2320: 2.0413198130946726\n",
      "Cost after epoch 2330: 2.0412944488039235\n",
      "Cost after epoch 2340: 2.041269893984458\n",
      "Cost after epoch 2350: 2.0412461825307004\n",
      "Cost after epoch 2360: 2.0412237988413593\n",
      "Cost after epoch 2370: 2.041202339857398\n",
      "Cost after epoch 2380: 2.0411817796762484\n",
      "Cost after epoch 2390: 2.041162144026407\n",
      "Cost after epoch 2400: 2.041143457611544\n",
      "Cost after epoch 2410: 2.041126100281193\n",
      "Cost after epoch 2420: 2.0411097398531215\n",
      "Cost after epoch 2430: 2.041094350547155\n",
      "Cost after epoch 2440: 2.041079947329246\n",
      "Cost after epoch 2450: 2.0410665436735203\n",
      "Cost after epoch 2460: 2.0410544099826313\n",
      "Cost after epoch 2470: 2.0410432882377565\n",
      "Cost after epoch 2480: 2.041033151909484\n",
      "Cost after epoch 2490: 2.041024004093825\n",
      "Cost after epoch 2500: 2.0410158461540835\n",
      "Cost after epoch 2510: 2.041008840738687\n",
      "Cost after epoch 2520: 2.041002803780939\n",
      "Cost after epoch 2530: 2.0409977077142147\n",
      "Cost after epoch 2540: 2.040993543841637\n",
      "Cost after epoch 2550: 2.0409903017159965\n",
      "Cost after epoch 2560: 2.0409880442890187\n",
      "Cost after epoch 2570: 2.0409866576007123\n",
      "Cost after epoch 2580: 2.0409861132253964\n",
      "Cost after epoch 2590: 2.0409863917658955\n",
      "Cost after epoch 2600: 2.0409874722398027\n",
      "Cost after epoch 2610: 2.04098933142212\n",
      "Cost after epoch 2620: 2.0409919181284257\n",
      "Cost after epoch 2630: 2.0409952035061116\n",
      "Cost after epoch 2640: 2.0409991592706533\n",
      "Cost after epoch 2650: 2.0410037558442955\n",
      "Cost after epoch 2660: 2.041008901498089\n",
      "Cost after epoch 2670: 2.041014597448218\n",
      "Cost after epoch 2680: 2.041020814889366\n",
      "Cost after epoch 2690: 2.0410275188656954\n",
      "Cost after epoch 2700: 2.041034673491846\n",
      "Cost after epoch 2710: 2.0410421388931863\n",
      "Cost after epoch 2720: 2.0410499559413866\n",
      "Cost after epoch 2730: 2.0410580962356666\n",
      "Cost after epoch 2740: 2.0410665204912397\n",
      "Cost after epoch 2750: 2.0410751888828758\n",
      "Cost after epoch 2760: 2.0410839349962986\n",
      "Cost after epoch 2770: 2.0410928250527762\n",
      "Cost after epoch 2780: 2.0411018312054114\n",
      "Cost after epoch 2790: 2.0411109120982824\n",
      "Cost after epoch 2800: 2.041120026214587\n",
      "Cost after epoch 2810: 2.041129002165596\n",
      "Cost after epoch 2820: 2.041137916501959\n",
      "Cost after epoch 2830: 2.041146741842015\n",
      "Cost after epoch 2840: 2.0411554367885123\n",
      "Cost after epoch 2850: 2.0411639601293445\n",
      "Cost after epoch 2860: 2.0411721559506035\n",
      "Cost after epoch 2870: 2.041180096199621\n",
      "Cost after epoch 2880: 2.0411877536475034\n",
      "Cost after epoch 2890: 2.041195088571746\n",
      "Cost after epoch 2900: 2.0412020617325113\n",
      "Cost after epoch 2910: 2.0412085511882125\n",
      "Cost after epoch 2920: 2.0412146101377946\n",
      "Cost after epoch 2930: 2.04122021101012\n",
      "Cost after epoch 2940: 2.041225317119124\n",
      "Cost after epoch 2950: 2.041229892502562\n",
      "Cost after epoch 2960: 2.041233865140898\n",
      "Cost after epoch 2970: 2.0412372566925088\n",
      "Cost after epoch 2980: 2.0412400386210217\n",
      "Cost after epoch 2990: 2.041242178276853\n",
      "Cost after epoch 3000: 2.041243643916537\n",
      "Cost after epoch 3010: 2.041244426473523\n",
      "Cost after epoch 3020: 2.0412445050434793\n",
      "Cost after epoch 3030: 2.0412438494318765\n",
      "Cost after epoch 3040: 2.0412424316806077\n",
      "Cost after epoch 3050: 2.0412402248626798\n",
      "Cost after epoch 3060: 2.0412372928061795\n",
      "Cost after epoch 3070: 2.0412335630623324\n",
      "Cost after epoch 3080: 2.041229003070905\n",
      "Cost after epoch 3090: 2.041223589908217\n",
      "Cost after epoch 3100: 2.041217301753838\n",
      "Cost after epoch 3110: 2.0412102821830076\n",
      "Cost after epoch 3120: 2.041202400395935\n",
      "Cost after epoch 3130: 2.0411936207933077\n",
      "Cost after epoch 3140: 2.04118392556017\n",
      "Cost after epoch 3150: 2.0411732980105075\n",
      "Cost after epoch 3160: 2.0411619653680324\n",
      "Cost after epoch 3170: 2.0411497338670332\n",
      "Cost after epoch 3180: 2.0411365642701025\n",
      "Cost after epoch 3190: 2.041122443725536\n",
      "Cost after epoch 3200: 2.041107360497388\n",
      "Cost after epoch 3210: 2.0410916266504144\n",
      "Cost after epoch 3220: 2.0410749829485946\n",
      "Cost after epoch 3230: 2.0410573860223113\n",
      "Cost after epoch 3240: 2.0410388276682068\n",
      "Cost after epoch 3250: 2.0410193007546598\n",
      "Cost after epoch 3260: 2.040999200986147\n",
      "Cost after epoch 3270: 2.040978203105417\n",
      "Cost after epoch 3280: 2.040956259242168\n",
      "Cost after epoch 3290: 2.0409333654055906\n",
      "Cost after epoch 3300: 2.040909518609641\n",
      "Cost after epoch 3310: 2.040885194961383\n",
      "Cost after epoch 3320: 2.0408600043733167\n",
      "Cost after epoch 3330: 2.040833894242913\n",
      "Cost after epoch 3340: 2.040806864279504\n",
      "Cost after epoch 3350: 2.040778915114622\n",
      "Cost after epoch 3360: 2.0407505983608822\n",
      "Cost after epoch 3370: 2.04072146178725\n",
      "Cost after epoch 3380: 2.0406914479670637\n",
      "Cost after epoch 3390: 2.040660559759211\n",
      "Cost after epoch 3400: 2.0406288008530837\n",
      "Cost after epoch 3410: 2.0405967921829262\n",
      "Cost after epoch 3420: 2.0405640232933684\n",
      "Cost after epoch 3430: 2.040530431976719\n",
      "Cost after epoch 3440: 2.040496023684304\n",
      "Cost after epoch 3450: 2.0404608046025796\n",
      "Cost after epoch 3460: 2.0404254578707\n",
      "Cost after epoch 3470: 2.0403894196951087\n",
      "Cost after epoch 3480: 2.0403526232546527\n",
      "Cost after epoch 3490: 2.040315076054164\n",
      "Cost after epoch 3500: 2.040276786238853\n",
      "Cost after epoch 3510: 2.0402384914270324\n",
      "Cost after epoch 3520: 2.0401995800840313\n",
      "Cost after epoch 3530: 2.040159981052405\n",
      "Cost after epoch 3540: 2.04011970338772\n",
      "Cost after epoch 3550: 2.0400787566950775\n",
      "Cost after epoch 3560: 2.040037925023339\n",
      "Cost after epoch 3570: 2.0399965551669967\n",
      "Cost after epoch 3580: 2.0399545720013834\n",
      "Cost after epoch 3590: 2.039911985678178\n",
      "Cost after epoch 3600: 2.0398688068139514\n",
      "Cost after epoch 3610: 2.0398258577615818\n",
      "Cost after epoch 3620: 2.039782449974789\n",
      "Cost after epoch 3630: 2.0397385048035432\n",
      "Cost after epoch 3640: 2.039694033095996\n",
      "Cost after epoch 3650: 2.0396490460881966\n",
      "Cost after epoch 3660: 2.0396043964354074\n",
      "Cost after epoch 3670: 2.0395593666548906\n",
      "Cost after epoch 3680: 2.039513875066557\n",
      "Cost after epoch 3690: 2.0394679328730403\n",
      "Cost after epoch 3700: 2.039421551596266\n",
      "Cost after epoch 3710: 2.0393756064791835\n",
      "Cost after epoch 3720: 2.039329357424969\n",
      "Cost after epoch 3730: 2.0392827202536297\n",
      "Cost after epoch 3740: 2.03923570623737\n",
      "Cost after epoch 3750: 2.0391883269076834\n",
      "Cost after epoch 3760: 2.039141472791555\n",
      "Cost after epoch 3770: 2.039094387291472\n",
      "Cost after epoch 3780: 2.0390469842754166\n",
      "Cost after epoch 3790: 2.038999274854444\n",
      "Cost after epoch 3800: 2.038951270347303\n",
      "Cost after epoch 3810: 2.0389038697624025\n",
      "Cost after epoch 3820: 2.0388563058066986\n",
      "Cost after epoch 3830: 2.0388084909465585\n",
      "Cost after epoch 3840: 2.0387604359509224\n",
      "Cost after epoch 3850: 2.038712151752751\n",
      "Cost after epoch 3860: 2.0386645396004894\n",
      "Cost after epoch 3870: 2.038616826928738\n",
      "Cost after epoch 3880: 2.0385689253422106\n",
      "Cost after epoch 3890: 2.038520845131158\n",
      "Cost after epoch 3900: 2.0384725967134405\n",
      "Cost after epoch 3910: 2.0384250779322657\n",
      "Cost after epoch 3920: 2.038377515940025\n",
      "Cost after epoch 3930: 2.038329821998627\n",
      "Cost after epoch 3940: 2.0382820058216904\n",
      "Cost after epoch 3950: 2.038234077220525\n",
      "Cost after epoch 3960: 2.0381869255978975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 3970: 2.038139782349736\n",
      "Cost after epoch 3980: 2.038092558884505\n",
      "Cost after epoch 3990: 2.038045264274071\n",
      "Cost after epoch 4000: 2.037997907663769\n",
      "Cost after epoch 4010: 2.037951365587944\n",
      "Cost after epoch 4020: 2.0379048777326036\n",
      "Cost after epoch 4030: 2.0378583561063994\n",
      "Cost after epoch 4040: 2.0378118091015884\n",
      "Cost after epoch 4050: 2.037765245164577\n",
      "Cost after epoch 4060: 2.0377195241246\n",
      "Cost after epoch 4070: 2.0376738975238537\n",
      "Cost after epoch 4080: 2.0376282783847746\n",
      "Cost after epoch 4090: 2.037582674404277\n",
      "Cost after epoch 4100: 2.037537093318254\n",
      "Cost after epoch 4110: 2.037492374979686\n",
      "Cost after epoch 4120: 2.0374477858822\n",
      "Cost after epoch 4130: 2.0374032404329885\n",
      "Cost after epoch 4140: 2.037358745635386\n",
      "Cost after epoch 4150: 2.0373143085199987\n",
      "Cost after epoch 4160: 2.037270746225226\n",
      "Cost after epoch 4170: 2.0372273428383405\n",
      "Cost after epoch 4180: 2.0371840144784934\n",
      "Cost after epoch 4190: 2.037140767470645\n",
      "Cost after epoch 4200: 2.0370976081581604\n",
      "Cost after epoch 4210: 2.037055328722027\n",
      "Cost after epoch 4220: 2.037013233056259\n",
      "Cost after epoch 4230: 2.036971239275448\n",
      "Cost after epoch 4240: 2.0369293530513577\n",
      "Cost after epoch 4250: 2.0368875800675816\n",
      "Cost after epoch 4260: 2.0368466857603376\n",
      "Cost after epoch 4270: 2.036805995642473\n",
      "Cost after epoch 4280: 2.0367654300647873\n",
      "Cost after epoch 4290: 2.0367249940780785\n",
      "Cost after epoch 4300: 2.036684692740221\n",
      "Cost after epoch 4310: 2.0366452633696435\n",
      "Cost after epoch 4320: 2.0366060545398383\n",
      "Cost after epoch 4330: 2.0365669890376177\n",
      "Cost after epoch 4340: 2.03652807132969\n",
      "Cost after epoch 4350: 2.036489305886516\n",
      "Cost after epoch 4360: 2.036451400909022\n",
      "Cost after epoch 4370: 2.0364137291349262\n",
      "Cost after epoch 4380: 2.0363762159481844\n",
      "Cost after epoch 4390: 2.036338865271006\n",
      "Cost after epoch 4400: 2.036301681027132\n",
      "Cost after epoch 4410: 2.0362653416335363\n",
      "Cost after epoch 4420: 2.0362292447898853\n",
      "Cost after epoch 4430: 2.0361933186024004\n",
      "Cost after epoch 4440: 2.0361575664895715\n",
      "Cost after epoch 4450: 2.036121991870033\n",
      "Cost after epoch 4460: 2.0360872430052765\n",
      "Cost after epoch 4470: 2.036052743080654\n",
      "Cost after epoch 4480: 2.0360184230166585\n",
      "Cost after epoch 4490: 2.0359842857688695\n",
      "Cost after epoch 4500: 2.0359503342922243\n",
      "Cost after epoch 4510: 2.035917186579903\n",
      "Cost after epoch 4520: 2.035884291583501\n",
      "Cost after epoch 4530: 2.035851583099694\n",
      "Cost after epoch 4540: 2.035819063661106\n",
      "Cost after epoch 4550: 2.0357867357993764\n",
      "Cost after epoch 4560: 2.035755187351437\n",
      "Cost after epoch 4570: 2.035723893101809\n",
      "Cost after epoch 4580: 2.0356927897582153\n",
      "Cost after epoch 4590: 2.035661879468808\n",
      "Cost after epoch 4600: 2.035631164380717\n",
      "Cost after epoch 4610: 2.0356012024803776\n",
      "Cost after epoch 4620: 2.0355714942655787\n",
      "Cost after epoch 4630: 2.035541979365885\n",
      "Cost after epoch 4640: 2.035512659581569\n",
      "Cost after epoch 4650: 2.03548353671205\n",
      "Cost after epoch 4660: 2.0354551393641236\n",
      "Cost after epoch 4670: 2.0354269934684064\n",
      "Cost after epoch 4680: 2.0353990415659347\n",
      "Cost after epoch 4690: 2.035371285143542\n",
      "Cost after epoch 4700: 2.0353437256875098\n",
      "Cost after epoch 4710: 2.0353168630349714\n",
      "Cost after epoch 4720: 2.0352902481317026\n",
      "Cost after epoch 4730: 2.0352638264014185\n",
      "Cost after epoch 4740: 2.0352375990496667\n",
      "Cost after epoch 4750: 2.035211567281817\n",
      "Cost after epoch 4760: 2.035186202890994\n",
      "Cost after epoch 4770: 2.035161081304729\n",
      "Cost after epoch 4780: 2.035136150784778\n",
      "Cost after epoch 4790: 2.035111412285227\n",
      "Cost after epoch 4800: 2.0350868667603934\n",
      "Cost after epoch 4810: 2.035062958779665\n",
      "Cost after epoch 4820: 2.0350392876226753\n",
      "Cost after epoch 4830: 2.0350158043311373\n",
      "Cost after epoch 4840: 2.0349925096351984\n",
      "Cost after epoch 4850: 2.0349694042656568\n",
      "Cost after epoch 4860: 2.0349469064642394\n",
      "Cost after epoch 4870: 2.034924638654358\n",
      "Cost after epoch 4880: 2.0349025545883426\n",
      "Cost after epoch 4890: 2.0348806547976968\n",
      "Cost after epoch 4900: 2.034858939814996\n",
      "Cost after epoch 4910: 2.0348378025094807\n",
      "Cost after epoch 4920: 2.0348168876770645\n",
      "Cost after epoch 4930: 2.0347961517021123\n",
      "Cost after epoch 4940: 2.0347755949406623\n",
      "Cost after epoch 4950: 2.034755217750227\n",
      "Cost after epoch 4960: 2.0347353886270456\n",
      "Cost after epoch 4970: 2.0347157739193364\n",
      "Cost after epoch 4980: 2.034696332557479\n",
      "Cost after epoch 4990: 2.0346770647432315\n",
      "training time: 0:00:05.544943\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "paras = skipgram_model_training(X, Y, vocab_size, 50, 0.05, 5000, batch_size=128, parameters=None, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.arange(vocab_size)\n",
    "X_test = np.expand_dims(X_test, axis=0)\n",
    "softmax_test, _ = forward_propagation(X_test, paras)\n",
    "top_sorted_inds = np.argsort(softmax_test, axis=0)[-4:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deduction's neighbor words: ['costs', 'after', 'the', 'beating']\n",
      "stock's neighbor words: ['investing', 'a', 'is', 'market']\n",
      "after's neighbor words: ['of', 'the', 'deduction', 'costs']\n",
      "game's neighbor words: ['is', \"loser's\", 'a', 'beating']\n",
      "the's neighbor words: ['is', 'of', 'beating', 'stock']\n",
      "market's neighbor words: ['is', \"loser's\", 'a', 'beating']\n",
      "is's neighbor words: ['game', 'market', \"loser's\", 'stock']\n",
      "of's neighbor words: ['the', 'of', 'costs', 'beating']\n",
      "costs's neighbor words: ['beating', 'of', 'deduction', 'the']\n",
      "loser's's neighbor words: ['game', 'market', 'stock', \"loser's\"]\n",
      "beating's neighbor words: ['market', 'stock', 'investing', 'costs']\n",
      "investing's neighbor words: ['the', 'stock', 'beating', \"loser's\"]\n",
      "a's neighbor words: ['game', 'market', \"loser's\", 'stock']\n"
     ]
    }
   ],
   "source": [
    "for input_ind in range(vocab_size):\n",
    "    input_word = id_to_word[input_ind]\n",
    "    output_words = [id_to_word[output_ind] for output_ind in top_sorted_inds[::-1, input_ind]]\n",
    "    print(\"{}'s neighbor words: {}\".format(input_word, output_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
