{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_wrd_emb(vocab_size, emb_size):\n",
    "    WRD_EMB = np.random.randn(vocab_size, emb_size) * 0.01\n",
    "    return WRD_EMB\n",
    "\n",
    "def initialize_dense(input_size, output_size):\n",
    "    W = np.random.randn(output_size, input_size) * 0.01\n",
    "    b = np.random.randn(output_size, 1) * 0.01\n",
    "    return W, b\n",
    "\n",
    "def initialize_parameters(vocab_size, emb_size):\n",
    "    WRD_EMB = initialize_wrd_emb(vocab_size, emb_size)\n",
    "    W, b = initialize_dense(emb_size, vocab_size)\n",
    "    \n",
    "    parameters = {}\n",
    "    parameters['WRD_EMB'] = WRD_EMB\n",
    "    parameters['W'] = W\n",
    "    parameters['b'] = b\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_to_word_vecs(inds, parameters):\n",
    "    \"\"\"\n",
    "    inds -- shape: (CBOW_N, number of examples)\n",
    "    \"\"\"\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    word_vecs = np.take(WRD_EMB, inds.T, axis=0).T\n",
    "    \n",
    "    assert(word_vecs.shape == (WRD_EMB.shape[1], inds.shape[0], inds.shape[1]))\n",
    "    \n",
    "    return word_vecs\n",
    "\n",
    "def mean_(word_vecs):\n",
    "    word_vecs_mean = np.mean(word_vecs, axis=1)\n",
    "    word_vecs_mean = word_vecs_mean.reshape(word_vecs.shape[0], -1)\n",
    "    \n",
    "    assert(word_vecs_mean.shape == (word_vecs.shape[0], word_vecs.shape[2]))\n",
    "    \n",
    "    return word_vecs_mean\n",
    "\n",
    "def linear_dense(word_vecs_mean, parameters):\n",
    "    W, b = parameters['W'], parameters['b']\n",
    "    Z = np.dot(W, word_vecs_mean) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], word_vecs_mean.shape[1]))\n",
    "    \n",
    "    return W, b, Z\n",
    "\n",
    "def softmax(Z):\n",
    "    softmax_out = np.divide(np.exp(Z), np.sum(np.exp(Z), axis=0, keepdims=True) + 0.001)\n",
    "    \n",
    "    assert(softmax_out.shape == Z.shape)\n",
    "\n",
    "    return softmax_out\n",
    "\n",
    "def forward_propagation(inds, parameters):\n",
    "    word_vecs = ind_to_word_vecs(inds, parameters)\n",
    "    word_vecs_mean = mean_(word_vecs)\n",
    "    W, b, Z = linear_dense(word_vecs_mean, parameters)\n",
    "    softmax_out = softmax(Z)\n",
    "    \n",
    "    caches = {}\n",
    "    caches['inds'] = inds\n",
    "    caches['word_vecs'] = word_vecs\n",
    "    caches['word_vecs_mean'] = word_vecs_mean\n",
    "    caches['W'] = W\n",
    "    caches['b'] = b\n",
    "    caches['Z'] = Z\n",
    "    \n",
    "    return softmax_out, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(softmax_out, Y):\n",
    "    m = softmax_out.shape[1]\n",
    "    cost = -(1 / m) * np.sum(np.sum(Y * np.log(softmax_out + 0.001), axis=0, keepdims=True), axis=1)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(Y, caches):\n",
    "    Z = caches['Z']\n",
    "    dL_dZ = Z - Y\n",
    "    \n",
    "    assert(dL_dZ.shape == Z.shape)\n",
    "    \n",
    "    return dL_dZ\n",
    "\n",
    "def dense_backward(dL_dZ, caches):\n",
    "    \"\"\"\n",
    "    Z = W * X + b\n",
    "    \"\"\"\n",
    "    W = caches['W']\n",
    "    b = caches['b']\n",
    "    word_vecs_mean = caches['word_vecs_mean']\n",
    "    m = word_vecs_mean.shape[1]\n",
    "    \n",
    "    dL_dW = (1 / m) * np.dot(dL_dZ, word_vecs_mean.T)\n",
    "    dL_db = (1 / m) * np.sum(dL_dZ, axis=1, keepdims=True)\n",
    "    dL_dword_vecs_mean = np.dot(W.T, dL_dZ)\n",
    "\n",
    "    assert(W.shape == dL_dW.shape)\n",
    "    assert(b.shape == dL_db.shape)\n",
    "    assert(word_vecs_mean.shape == dL_dword_vecs_mean.shape)\n",
    "    \n",
    "    return dL_dW, dL_db, dL_dword_vecs_mean\n",
    "\n",
    "def mean_backward(dL_dword_vecs_mean, caches):\n",
    "    \"\"\"\n",
    "    ele_mean_i = (1 / n) * (ele_1 + ele_2 + ele_n)\n",
    "    \"\"\"\n",
    "    word_vecs = caches['word_vecs']\n",
    "    CBOW_N = word_vecs.shape[1]\n",
    "    \n",
    "#     dL_dword_vecs = (1 / m) * (1 / CBOW_N) * np.ones((dL_dword_vecs_mean.shape[0], CBOW_N)) *\\\n",
    "#         np.sum(dL_dword_vecs_mean, axis=1, keepdims=True)\n",
    "        \n",
    "    dL_dword_vecs = (1 / m) * (1 / CBOW_N) * np.sum(dL_dword_vecs_mean, axis=1, keepdims=True)\n",
    "    assert((word_vecs.shape[0], 1) == dL_dword_vecs.shape)\n",
    "    \n",
    "    return dL_dword_vecs\n",
    "\n",
    "def backward_propagation(Y, caches):\n",
    "    dL_dZ = softmax_backward(Y, caches)\n",
    "    dL_dW, dL_db, dL_dword_vecs_mean = dense_backward(dL_dZ, caches)\n",
    "    dL_dword_vecs = mean_backward(dL_dword_vecs_mean, caches)\n",
    "    \n",
    "    gradients = dict()\n",
    "    gradients['dL_dZ'] = dL_dZ\n",
    "    gradients['dL_dW'] = dL_dW\n",
    "    gradients['dL_db'] = dL_db\n",
    "    gradients['dL_dword_vecs'] = dL_dword_vecs\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, caches, gradients, learning_rate):\n",
    "    CBOW_N = caches['inds'].shape[0]\n",
    "    vocab_size, emb_size = parameters['WRD_EMB'].shape\n",
    "    inds = caches['inds']\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    dL_dword_vecs = gradients['dL_dword_vecs']\n",
    "    m = inds.shape[-1]\n",
    "    \n",
    "    \n",
    "#     updated_WRD_EMD = parameters['WRD_EMB'][inds.T, :] -\\\n",
    "#         learning_rate * gradients['dL_dword_vecs'].T.reshape(1, CBOW_N, -1)\n",
    "    for i in range(m):\n",
    "        WRD_EMB[inds[:, i], :] -= dL_dword_vecs.T * learning_rate\n",
    "    \n",
    "#     parameters['WRD_EMB'][inds.flatten(), :] = updated_WRD_EMD.reshape(-1, emb_size)\n",
    "    parameters['W'] -= learning_rate * gradients['dL_dW']\n",
    "    parameters['b'] -= learning_rate * gradients['dL_db']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import getsizeof\n",
    "import gc\n",
    "\n",
    "\n",
    "def cbow_model(X, Y, vocab_size, emb_size, learning_rate, epochs, batch_size=256, parameters=None, print_cost=False):\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    if parameters is None:\n",
    "        parameters = initialize_parameters(vocab_size, emb_size)\n",
    "\n",
    "#     batch_inds = list(range(0, m, batch_size))\n",
    "    for epoch in range(epochs):\n",
    "#         np.random.shuffle(batch_inds)\n",
    "        for b, i in enumerate(range(0, m, batch_size)):\n",
    "            X_batch = X[:, i:i+batch_size]\n",
    "            Y_batch = Y[:, i:i+batch_size]\n",
    "\n",
    "            softmax_out, caches = forward_propagation(X_batch, parameters)\n",
    "            gradients = backward_propagation(Y_batch, caches)\n",
    "            update_parameters(parameters, caches, gradients, learning_rate)\n",
    "            cost = cross_entropy(softmax_out, Y_batch)\n",
    "#             if b % 100 == 0:\n",
    "#                 print('epoch {}, {}/{} - Cost: {}'.format(epoch, b, 1000, np.squeeze(cost)))\n",
    "#             gc.collect()\n",
    "#             del X_batch\n",
    "#             del Y_batch\n",
    "#             del caches\n",
    "#             del gradients\n",
    "            \n",
    "            \n",
    "        costs.append(cost)\n",
    "        if print_cost and epoch % 1000 == 0:\n",
    "            print(\"Cost after epoch {}: {}\".format(epoch, np.squeeze(cost)))\n",
    "        \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence: I(0) would(1) like(2) to(3) get(4) a(5) better(6) job(7).\n",
    "vocab_size = 8\n",
    "\n",
    "[0, 2] [1]  \n",
    "[1, 3] [2]  \n",
    "[2, 4] [3]  \n",
    "[3, 5] [4]  \n",
    "[4, 6] [5]  \n",
    "[5, 7] [6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters(31295, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Probagation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-60406cc4cb14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_out, caches = forward_propagation(X[:, 0].reshape(-1, 1), parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(softmax_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Cost Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = cross_entropy(softmax_out, Y_one_hot[:, 0].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Probagation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = backward_propagation(Y_one_hot[:, 0].reshape(-1, 1), caches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_model(X[:, 0].reshape(-1, 1), Y_one_hot[:, 0].reshape(-1, 1), 31295, 100, 0.05, 1, batch_size=256, parameters=None, print_cost=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack Overflow data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('pickles/X.pkl', 'rb') as file:\n",
    "    X = pickle.load(file)\n",
    "    \n",
    "with open('pickles/Y.pkl', 'rb') as file:\n",
    "    Y = pickle.load(file)\n",
    "    \n",
    "assert(X.shape[-1] == Y.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 31295\n",
    "m = X.shape[-1]\n",
    "batch_size = 256\n",
    "emb_size = 100\n",
    "\n",
    "Y_one_hot = np.zeros((vocab_size, m))\n",
    "Y_one_hot[Y.flatten(), np.arange(m)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ran_sam_inds = np.random.randint(m, size=2)\n",
    "X_sample, Y_one_hot_sample = X[:, np.arange(10000)], Y_one_hot[:, np.arange(10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_sample.shape, Y_one_hot_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = cbow_model(X, Y_one_hot, vocab_size, emb_size, 0.05, 2000, batch_size=126, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRD_EMB = parameters['WRD_EMB']\n",
    "\n",
    "# with open('pickles/word_to_id.pkl', 'rb') as file:\n",
    "#     word_to_id = pickle.load(file)\n",
    "    \n",
    "# with open('pickles/id_to_word.pkl', 'rb') as file:\n",
    "#     id_to_word = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_n_similar(word, wrd_emb, n=10):\n",
    "    id_ = word_to_id[word]\n",
    "    vec_word = wrd_emb[id_, :]\n",
    "    norm_vec_word = np.linalg.norm(vec_word)\n",
    "    cos_sim = np.dot(wrd_emb, vec_word.T) / (np.linalg.norm(wrd_emb, axis=1) * norm_vec_word)\n",
    "    top_n_ind = np.argsort(cos_sim)[-n:][::-1]\n",
    "    return top_n_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = find_top_n_similar('sort', WRD_EMB, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[id_to_word[id_] for id_ in inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word[156]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id['bfs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Data: the quick brown fox jumped over the lazy dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_toy = ['the','quick','brown','fox','jumped','over','the','lazy','dog']\n",
    "id_to_word_toy = dict()\n",
    "word_to_id_toy = dict()\n",
    "i = 0\n",
    "for token in text_toy:\n",
    "    if token in word_to_id_toy:\n",
    "        continue\n",
    "    id_to_word_toy[i] = token\n",
    "    word_to_id_toy[token] = i\n",
    "    i += 1\n",
    "window_size = 1\n",
    "example_len = 2 * 1 + 1\n",
    "X_toy = []\n",
    "Y_toy = []\n",
    "\n",
    "for i in range(len(text_toy) - example_len + 1):\n",
    "    X_toy.extend([word_to_id_toy[word] for word in text_toy[i:i+1] + text_toy[i+2:i+3]])\n",
    "    Y_toy.append(word_to_id_toy[text_toy[i+1]])\n",
    "        \n",
    "X_toy = np.array(X_toy)\n",
    "X_toy = X_toy.reshape(-1, window_size * 2).T\n",
    "Y_toy = np.array(Y_toy)\n",
    "Y_toy = Y_toy.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 8\n",
    "m = X_toy.shape[-1]\n",
    "emb_size = 6\n",
    "\n",
    "Y_toy_one_hot = np.zeros((vocab_size, m))\n",
    "Y_toy_one_hot[Y_toy.flatten(), np.arange(m)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 7) (8, 7)\n"
     ]
    }
   ],
   "source": [
    "print(X_toy.shape, Y_toy_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (31295,7) (8,7) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-34169b517cf0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbow_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_toy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_toy_one_hot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memb_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0025\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10000000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m126\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_cost\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-6b2b63a973bb>\u001b[0m in \u001b[0;36mcbow_model\u001b[1;34m(X, Y, vocab_size, emb_size, learning_rate, epochs, batch_size, parameters, print_cost)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0msoftmax_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0mupdate_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoftmax_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-420077a3909a>\u001b[0m in \u001b[0;36mbackward_propagation\u001b[1;34m(Y, caches)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbackward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mdL_dZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[0mdL_dW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdL_db\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdL_dword_vecs_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdense_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdL_dZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mdL_dword_vecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdL_dword_vecs_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-420077a3909a>\u001b[0m in \u001b[0;36msoftmax_backward\u001b[1;34m(Y, caches)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msoftmax_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcaches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Z'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdL_dZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mZ\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdL_dZ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mZ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (31295,7) (8,7) "
     ]
    }
   ],
   "source": [
    "parameters = cbow_model(X_toy, Y_toy_one_hot, vocab_size, emb_size, 0.0025, 10000000, parameters=parameters, batch_size=126, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRD_EMB = parameters['WRD_EMB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_n_similar(word, wrd_emb, n=10):\n",
    "    id_ = word_to_id_toy[word]\n",
    "    vec_word = wrd_emb[id_, :]\n",
    "    norm_vec_word = np.linalg.norm(vec_word)\n",
    "    cos_sim = np.dot(wrd_emb, vec_word.T) / (np.linalg.norm(wrd_emb, axis=1) * norm_vec_word)\n",
    "    top_n_ind = np.argsort(cos_sim)[-n:][::-1]\n",
    "    return [id_to_word_toy[id_] for id_ in top_n_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_top_n_similar('lazy', WRD_EMB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
