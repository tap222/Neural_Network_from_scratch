{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # obtains tokens with a least 1 alphabet\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())\n",
    "\n",
    "def mapping(tokens):\n",
    "    word_to_id = dict()\n",
    "    id_to_word = dict()\n",
    "\n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "def generate_training_data(tokens, word_to_id, window_size):\n",
    "    N = len(tokens)\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(N):\n",
    "        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n",
    "                   list(range(i + 1, min(N, i + window_size + 1)))\n",
    "        for j in nbr_inds:\n",
    "            X.append(word_to_id[tokens[i]])\n",
    "            Y.append(word_to_id[tokens[j]])\n",
    "            \n",
    "    X = np.array(X)\n",
    "    X = np.expand_dims(X, axis=0)\n",
    "    Y = np.array(Y)\n",
    "    Y = np.expand_dims(Y, axis=0)\n",
    "            \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"After the deduction of the costs of investing, \" \\\n",
    "      \"beating the stock market is a loser's game.\"\n",
    "tokens = tokenize(doc)\n",
    "word_to_id, id_to_word = mapping(tokens)\n",
    "X, Y = generate_training_data(tokens, word_to_id, 3)\n",
    "vocab_size = len(id_to_word)\n",
    "m = Y.shape[1]\n",
    "# turn Y into one hot encoding\n",
    "Y_one_hot = np.zeros((vocab_size, m))\n",
    "Y_one_hot[Y.flatten(), np.arange(m)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_wrd_emb(vocab_size, emb_size):\n",
    "    \"\"\"\n",
    "    vocab_size: int. vocabulary size of your corpus or training data\n",
    "    emb_size: int. word embedding size. How many dimensions to represent each vocabulary\n",
    "    \"\"\"\n",
    "    WRD_EMB = np.random.randn(vocab_size, emb_size) * 0.01\n",
    "    \n",
    "    assert(WRD_EMB.shape == (vocab_size, emb_size))\n",
    "    return WRD_EMB\n",
    "\n",
    "def initialize_dense(input_size, output_size):\n",
    "    \"\"\"\n",
    "    input_size: int. size of the input to the dense layer\n",
    "    output_szie: int. size of the output out of the dense layer\n",
    "    \"\"\"\n",
    "    W = np.random.randn(output_size, input_size) * 0.01\n",
    "    \n",
    "    assert(W.shape == (output_size, input_size))\n",
    "    return W\n",
    "\n",
    "def initialize_parameters(vocab_size, emb_size):\n",
    "    WRD_EMB = initialize_wrd_emb(vocab_size, emb_size)\n",
    "    W = initialize_dense(emb_size, vocab_size)\n",
    "    \n",
    "    parameters = {}\n",
    "    parameters['WRD_EMB'] = WRD_EMB\n",
    "    parameters['W'] = W\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_to_word_vecs(inds, parameters):\n",
    "    \"\"\"\n",
    "    inds: numpy array. shape: (1, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = inds.shape[1]\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    word_vec = WRD_EMB[inds.flatten(), :].T\n",
    "    \n",
    "    assert(word_vec.shape == (WRD_EMB.shape[1], m))\n",
    "    \n",
    "    return word_vec\n",
    "\n",
    "def linear_dense(word_vec, parameters):\n",
    "    \"\"\"\n",
    "    word_vec: numpy array. shape: (emb_size, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = word_vec.shape[1]\n",
    "    W = parameters['W']\n",
    "    Z = np.dot(W, word_vec)\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], m))\n",
    "    \n",
    "    return W, Z\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Z: output out of the dense layer. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    softmax_out = np.divide(np.exp(Z), np.sum(np.exp(Z), axis=0, keepdims=True) + 0.001)\n",
    "    \n",
    "    assert(softmax_out.shape == Z.shape)\n",
    "\n",
    "    return softmax_out\n",
    "\n",
    "def forward_propagation(inds, parameters):\n",
    "    word_vec = ind_to_word_vecs(inds, parameters)\n",
    "    W, Z = linear_dense(word_vec, parameters)\n",
    "    softmax_out = softmax(Z)\n",
    "    \n",
    "    caches = {}\n",
    "    caches['inds'] = inds\n",
    "    caches['word_vec'] = word_vec\n",
    "    caches['W'] = W\n",
    "    caches['Z'] = Z\n",
    "    \n",
    "    return softmax_out, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(softmax_out, Y):\n",
    "    \"\"\"\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    m = softmax_out.shape[1]\n",
    "    cost = -(1 / m) * np.sum(np.sum(Y * np.log(softmax_out + 0.001), axis=0, keepdims=True), axis=1)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(Y, softmax_out):\n",
    "    \"\"\"\n",
    "    Y: labels of training data. shape: (vocab_size, m)\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    dL_dZ = softmax_out - Y\n",
    "    \n",
    "    assert(dL_dZ.shape == softmax_out.shape)\n",
    "    return dL_dZ\n",
    "\n",
    "def dense_backward(dL_dZ, caches):\n",
    "    \"\"\"\n",
    "    dL_dZ: shape: (vocab_size, m)\n",
    "    caches: dict. results from each steps of forward propagation\n",
    "    \"\"\"\n",
    "    W = caches['W']\n",
    "    word_vec = caches['word_vec']\n",
    "    m = word_vec.shape[1]\n",
    "    \n",
    "    dL_dW = (1 / m) * np.dot(dL_dZ, word_vec.T)\n",
    "    dL_dword_vec = np.dot(W.T, dL_dZ)\n",
    "\n",
    "    assert(W.shape == dL_dW.shape)\n",
    "    assert(word_vec.shape == dL_dword_vec.shape)\n",
    "    \n",
    "    return dL_dW, dL_dword_vec\n",
    "\n",
    "def backward_propagation(Y, softmax_out, caches):\n",
    "    dL_dZ = softmax_backward(Y, softmax_out)\n",
    "    dL_dW, dL_dword_vec = dense_backward(dL_dZ, caches)\n",
    "    \n",
    "    gradients = dict()\n",
    "    gradients['dL_dZ'] = dL_dZ\n",
    "    gradients['dL_dW'] = dL_dW\n",
    "    gradients['dL_dword_vec'] = dL_dword_vec\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, caches, gradients, learning_rate):\n",
    "    vocab_size, emb_size = parameters['WRD_EMB'].shape\n",
    "    inds = caches['inds']\n",
    "    dL_dword_vec = gradients['dL_dword_vec']\n",
    "    m = inds.shape[-1]\n",
    "    \n",
    "    parameters['WRD_EMB'][inds.flatten(), :] -= dL_dword_vec.T * learning_rate\n",
    "\n",
    "    parameters['W'] -= learning_rate * gradients['dL_dW']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def skipgram_model_training(X, Y, vocab_size, emb_size, learning_rate, epochs, batch_size=256, parameters=None, print_cost=False, plot_cost=True):\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    if parameters is None:\n",
    "        parameters = initialize_parameters(vocab_size, emb_size)\n",
    "    \n",
    "    begin_time = datetime.now()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_cost = 0\n",
    "        batch_inds = list(range(0, m, batch_size))\n",
    "        np.random.shuffle(batch_inds)\n",
    "        for i in batch_inds:\n",
    "            X_batch = X[:, i:i+batch_size]\n",
    "            Y_batch = Y[:, i:i+batch_size]\n",
    "\n",
    "            softmax_out, caches = forward_propagation(X_batch, parameters)\n",
    "            gradients = backward_propagation(Y_batch, softmax_out, caches)\n",
    "            update_parameters(parameters, caches, gradients, learning_rate)\n",
    "            cost = cross_entropy(softmax_out, Y_batch)\n",
    "            epoch_cost += np.squeeze(cost)\n",
    "            \n",
    "        costs.append(epoch_cost)\n",
    "        if print_cost and epoch % (epochs // 500) == 0:\n",
    "            print(\"Cost after epoch {}: {}\".format(epoch, epoch_cost))\n",
    "        if epoch % (epochs // 100) == 0:\n",
    "            learning_rate *= 0.98\n",
    "    end_time = datetime.now()\n",
    "    print('training time: {}'.format(end_time - begin_time))\n",
    "            \n",
    "    if plot_cost:\n",
    "        plt.plot(np.arange(epochs), costs)\n",
    "        plt.xlabel('# of epochs')\n",
    "        plt.ylabel('cost')\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 2.5521345998144445\n",
      "Cost after epoch 10: 2.5518238607726165\n",
      "Cost after epoch 20: 2.5514986872503935\n",
      "Cost after epoch 30: 2.551140805645701\n",
      "Cost after epoch 40: 2.5507317787639856\n",
      "Cost after epoch 50: 2.5502518951139908\n",
      "Cost after epoch 60: 2.5496908842904875\n",
      "Cost after epoch 70: 2.549019818227364\n",
      "Cost after epoch 80: 2.5482126808858894\n",
      "Cost after epoch 90: 2.547241361364313\n",
      "Cost after epoch 100: 2.546074069902972\n",
      "Cost after epoch 110: 2.5447021607289755\n",
      "Cost after epoch 120: 2.5430715509870603\n",
      "Cost after epoch 130: 2.5411365091446254\n",
      "Cost after epoch 140: 2.538848675427265\n",
      "Cost after epoch 150: 2.5361543454755426\n",
      "Cost after epoch 160: 2.5330557295836584\n",
      "Cost after epoch 170: 2.5294558199393355\n",
      "Cost after epoch 180: 2.525285432748843\n",
      "Cost after epoch 190: 2.520479391350566\n",
      "Cost after epoch 200: 2.5149731839519514\n",
      "Cost after epoch 210: 2.5088259947707003\n",
      "Cost after epoch 220: 2.5019113015685686\n",
      "Cost after epoch 230: 2.494182081159017\n",
      "Cost after epoch 240: 2.485622409439897\n",
      "Cost after epoch 250: 2.476242183067025\n",
      "Cost after epoch 260: 2.466272309815763\n",
      "Cost after epoch 270: 2.4556477271871895\n",
      "Cost after epoch 280: 2.4444583760676246\n",
      "Cost after epoch 290: 2.432848130270226\n",
      "Cost after epoch 300: 2.420989657128918\n",
      "Cost after epoch 310: 2.4092883129847493\n",
      "Cost after epoch 320: 2.3977418517563334\n",
      "Cost after epoch 330: 2.386501713742447\n",
      "Cost after epoch 340: 2.3757186237317556\n",
      "Cost after epoch 350: 2.3655056810385338\n",
      "Cost after epoch 360: 2.356095548873297\n",
      "Cost after epoch 370: 2.347329219075606\n",
      "Cost after epoch 380: 2.3391517010032192\n",
      "Cost after epoch 390: 2.331484466821067\n",
      "Cost after epoch 400: 2.3242144373807365\n",
      "Cost after epoch 410: 2.3173304436993973\n",
      "Cost after epoch 420: 2.3105737978184733\n",
      "Cost after epoch 430: 2.3037949571724994\n",
      "Cost after epoch 440: 2.2968704721641076\n",
      "Cost after epoch 450: 2.289696255510927\n",
      "Cost after epoch 460: 2.282331066528082\n",
      "Cost after epoch 470: 2.2746147221542574\n",
      "Cost after epoch 480: 2.266509515958996\n",
      "Cost after epoch 490: 2.2580180611547247\n",
      "Cost after epoch 500: 2.249166724114406\n",
      "Cost after epoch 510: 2.240170092688105\n",
      "Cost after epoch 520: 2.23095093494522\n",
      "Cost after epoch 530: 2.2215620368461173\n",
      "Cost after epoch 540: 2.2120808595838213\n",
      "Cost after epoch 550: 2.2025872865175136\n",
      "Cost after epoch 560: 2.193329045065754\n",
      "Cost after epoch 570: 2.1842237702072924\n",
      "Cost after epoch 580: 2.1753167828825752\n",
      "Cost after epoch 590: 2.1666651256135996\n",
      "Cost after epoch 600: 2.158317584201479\n",
      "Cost after epoch 610: 2.1504553764685213\n",
      "Cost after epoch 620: 2.1429689719371776\n",
      "Cost after epoch 630: 2.135862342109111\n",
      "Cost after epoch 640: 2.129147620636354\n",
      "Cost after epoch 650: 2.1228292544998957\n",
      "Cost after epoch 660: 2.1170084891964462\n",
      "Cost after epoch 670: 2.1115706182349925\n",
      "Cost after epoch 680: 2.1064912986219126\n",
      "Cost after epoch 690: 2.101755193031835\n",
      "Cost after epoch 700: 2.0973446755557745\n",
      "Cost after epoch 710: 2.0933123650583116\n",
      "Cost after epoch 720: 2.0895644631492916\n",
      "Cost after epoch 730: 2.0860738878765512\n",
      "Cost after epoch 740: 2.0828224965450977\n",
      "Cost after epoch 750: 2.079792843070724\n",
      "Cost after epoch 760: 2.0770177894805784\n",
      "Cost after epoch 770: 2.0744310262624244\n",
      "Cost after epoch 780: 2.072013066290864\n",
      "Cost after epoch 790: 2.0697513207170513\n",
      "Cost after epoch 800: 2.0676342010580697\n",
      "Cost after epoch 810: 2.0656858863048657\n",
      "Cost after epoch 820: 2.0638612129833027\n",
      "Cost after epoch 830: 2.062147696734779\n",
      "Cost after epoch 840: 2.0605377110875027\n",
      "Cost after epoch 850: 2.0590243098756043\n",
      "Cost after epoch 860: 2.0576262301741726\n",
      "Cost after epoch 870: 2.0563123776459213\n",
      "Cost after epoch 880: 2.0550748720711063\n",
      "Cost after epoch 890: 2.0539092398143644\n",
      "Cost after epoch 900: 2.0528114199611136\n",
      "Cost after epoch 910: 2.0517959729452344\n",
      "Cost after epoch 920: 2.0508411207126\n",
      "Cost after epoch 930: 2.0499418587514375\n",
      "Cost after epoch 940: 2.049095601370597\n",
      "Cost after epoch 950: 2.0483000073597646\n",
      "Cost after epoch 960: 2.0475661574394386\n",
      "Cost after epoch 970: 2.0468786820031784\n",
      "Cost after epoch 980: 2.0462343721624596\n",
      "Cost after epoch 990: 2.0456317274868243\n",
      "Cost after epoch 1000: 2.045069377220058\n",
      "Cost after epoch 1010: 2.044555306676843\n",
      "Cost after epoch 1020: 2.0440787354825427\n",
      "Cost after epoch 1030: 2.0436375317247983\n",
      "Cost after epoch 1040: 2.0432307261920717\n",
      "Cost after epoch 1050: 2.0428573955088054\n",
      "Cost after epoch 1060: 2.0425226553298343\n",
      "Cost after epoch 1070: 2.042219123844248\n",
      "Cost after epoch 1080: 2.0419452550555905\n",
      "Cost after epoch 1090: 2.041700235794314\n",
      "Cost after epoch 1100: 2.041483246788808\n",
      "Cost after epoch 1110: 2.041296785584083\n",
      "Cost after epoch 1120: 2.0411360693426155\n",
      "Cost after epoch 1130: 2.0409998476957134\n",
      "Cost after epoch 1140: 2.040887270970274\n",
      "Cost after epoch 1150: 2.040797465025812\n",
      "Cost after epoch 1160: 2.040730706821201\n",
      "Cost after epoch 1170: 2.040684256109879\n",
      "Cost after epoch 1180: 2.0406570397909025\n",
      "Cost after epoch 1190: 2.04064813780265\n",
      "Cost after epoch 1200: 2.0406566113312583\n",
      "Cost after epoch 1210: 2.0406810515458726\n",
      "Cost after epoch 1220: 2.0407203293783907\n",
      "Cost after epoch 1230: 2.040773534630599\n",
      "Cost after epoch 1240: 2.0408397324961705\n",
      "Cost after epoch 1250: 2.0409179846912298\n",
      "Cost after epoch 1260: 2.041005777530891\n",
      "Cost after epoch 1270: 2.041103208492264\n",
      "Cost after epoch 1280: 2.0412095485922466\n",
      "Cost after epoch 1290: 2.0413239243242858\n",
      "Cost after epoch 1300: 2.041445473421819\n",
      "Cost after epoch 1310: 2.041571113916564\n",
      "Cost after epoch 1320: 2.0417018063562953\n",
      "Cost after epoch 1330: 2.04183701422045\n",
      "Cost after epoch 1340: 2.041975985381724\n",
      "Cost after epoch 1350: 2.0421179896851602\n",
      "Cost after epoch 1360: 2.042259819006029\n",
      "Cost after epoch 1370: 2.0424029654622378\n",
      "Cost after epoch 1380: 2.0425470810543827\n",
      "Cost after epoch 1390: 2.042691572188164\n",
      "Cost after epoch 1400: 2.0428358742016663\n",
      "Cost after epoch 1410: 2.042976984457667\n",
      "Cost after epoch 1420: 2.0431166473729516\n",
      "Cost after epoch 1430: 2.0432546894183776\n",
      "Cost after epoch 1440: 2.043390693694424\n",
      "Cost after epoch 1450: 2.043524275962855\n",
      "Cost after epoch 1460: 2.0436528587866816\n",
      "Cost after epoch 1470: 2.043778224176742\n",
      "Cost after epoch 1480: 2.0439003525693766\n",
      "Cost after epoch 1490: 2.044019004950587\n",
      "Cost after epoch 1500: 2.0441339751129575\n",
      "Cost after epoch 1510: 2.044243221399094\n",
      "Cost after epoch 1520: 2.0443484137446517\n",
      "Cost after epoch 1530: 2.0444496582468688\n",
      "Cost after epoch 1540: 2.0445468762578365\n",
      "Cost after epoch 1550: 2.044640017837131\n",
      "Cost after epoch 1560: 2.044727586960652\n",
      "Cost after epoch 1570: 2.044811049096518\n",
      "Cost after epoch 1580: 2.0448905974323397\n",
      "Cost after epoch 1590: 2.0449662765879113\n",
      "Cost after epoch 1600: 2.045038151719373\n",
      "Cost after epoch 1610: 2.0451052011563577\n",
      "Cost after epoch 1620: 2.0451686397998574\n",
      "Cost after epoch 1630: 2.045228700554635\n",
      "Cost after epoch 1640: 2.0452854993497938\n",
      "Cost after epoch 1650: 2.0453391620174517\n",
      "Cost after epoch 1660: 2.045389021288942\n",
      "Cost after epoch 1670: 2.045436030571432\n",
      "Cost after epoch 1680: 2.0454804134753855\n",
      "Cost after epoch 1690: 2.045522301412519\n",
      "Cost after epoch 1700: 2.045561825264841\n",
      "Cost after epoch 1710: 2.045598543116502\n",
      "Cost after epoch 1720: 2.0456331605609837\n",
      "Cost after epoch 1730: 2.045665851539436\n",
      "Cost after epoch 1740: 2.0456967166830022\n",
      "Cost after epoch 1750: 2.0457258484828875\n",
      "Cost after epoch 1760: 2.0457529279624653\n",
      "Cost after epoch 1770: 2.0457784407304627\n",
      "Cost after epoch 1780: 2.0458024878133156\n",
      "Cost after epoch 1790: 2.0458251125896783\n",
      "Cost after epoch 1800: 2.045846347039223\n",
      "Cost after epoch 1810: 2.045865941020857\n",
      "Cost after epoch 1820: 2.045884191991529\n",
      "Cost after epoch 1830: 2.0459011249477714\n",
      "Cost after epoch 1840: 2.045916722647624\n",
      "Cost after epoch 1850: 2.0459309575959743\n",
      "Cost after epoch 1860: 2.045943646100879\n",
      "Cost after epoch 1870: 2.045954925073225\n",
      "Cost after epoch 1880: 2.045964757583805\n",
      "Cost after epoch 1890: 2.0459730819180693\n",
      "Cost after epoch 1900: 2.045979830535163\n",
      "Cost after epoch 1910: 2.0459849233022913\n",
      "Cost after epoch 1920: 2.045988355510175\n",
      "Cost after epoch 1930: 2.045990052881141\n",
      "Cost after epoch 1940: 2.0459899368897334\n",
      "Cost after epoch 1950: 2.045987929129078\n",
      "Cost after epoch 1960: 2.0459841082326227\n",
      "Cost after epoch 1970: 2.045978337872993\n",
      "Cost after epoch 1980: 2.0459705333548244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 1990: 2.045960628904837\n",
      "Cost after epoch 2000: 2.0459485644295645\n",
      "Cost after epoch 2010: 2.0459346272170564\n",
      "Cost after epoch 2020: 2.0459185531832786\n",
      "Cost after epoch 2030: 2.0459002690125203\n",
      "Cost after epoch 2040: 2.0458797444022934\n",
      "Cost after epoch 2050: 2.045856958370154\n",
      "Cost after epoch 2060: 2.0458324326097523\n",
      "Cost after epoch 2070: 2.0458057772791354\n",
      "Cost after epoch 2080: 2.0457769421499568\n",
      "Cost after epoch 2090: 2.0457459427443525\n",
      "Cost after epoch 2100: 2.0457128049084923\n",
      "Cost after epoch 2110: 2.045678277112283\n",
      "Cost after epoch 2120: 2.045641848505509\n",
      "Cost after epoch 2130: 2.0456034925526922\n",
      "Cost after epoch 2140: 2.045563267792399\n",
      "Cost after epoch 2150: 2.045521241698778\n",
      "Cost after epoch 2160: 2.045478350314408\n",
      "Cost after epoch 2170: 2.0454339730747195\n",
      "Cost after epoch 2180: 2.0453880993789526\n",
      "Cost after epoch 2190: 2.0453408182276847\n",
      "Cost after epoch 2200: 2.045292224673634\n",
      "Cost after epoch 2210: 2.045243381691559\n",
      "Cost after epoch 2220: 2.045193578550839\n",
      "Cost after epoch 2230: 2.045142809074531\n",
      "Cost after epoch 2240: 2.0450911757981802\n",
      "Cost after epoch 2250: 2.0450387839724717\n",
      "Cost after epoch 2260: 2.0449867548500986\n",
      "Cost after epoch 2270: 2.0449343142952303\n",
      "Cost after epoch 2280: 2.0448814495368066\n",
      "Cost after epoch 2290: 2.0448282602504495\n",
      "Cost after epoch 2300: 2.0447748458363013\n",
      "Cost after epoch 2310: 2.0447223213212995\n",
      "Cost after epoch 2320: 2.0446698798036373\n",
      "Cost after epoch 2330: 2.044617493972025\n",
      "Cost after epoch 2340: 2.0445652478810783\n",
      "Cost after epoch 2350: 2.044513223093586\n",
      "Cost after epoch 2360: 2.044462476509829\n",
      "Cost after epoch 2370: 2.0444121986298502\n",
      "Cost after epoch 2380: 2.044362343523784\n",
      "Cost after epoch 2390: 2.044312971685206\n",
      "Cost after epoch 2400: 2.044264139785682\n",
      "Cost after epoch 2410: 2.044216811598764\n",
      "Cost after epoch 2420: 2.0441702007015503\n",
      "Cost after epoch 2430: 2.044124241909912\n",
      "Cost after epoch 2440: 2.0440789688629164\n",
      "Cost after epoch 2450: 2.044034410850826\n",
      "Cost after epoch 2460: 2.043991421272192\n",
      "Cost after epoch 2470: 2.0439492551471585\n",
      "Cost after epoch 2480: 2.0439078298657454\n",
      "Cost after epoch 2490: 2.0438671526856518\n",
      "Cost after epoch 2500: 2.0438272266220894\n",
      "Cost after epoch 2510: 2.0437887936675296\n",
      "Cost after epoch 2520: 2.0437511600232114\n",
      "Cost after epoch 2530: 2.0437142288911176\n",
      "Cost after epoch 2540: 2.043677984285076\n",
      "Cost after epoch 2550: 2.0436424065259433\n",
      "Cost after epoch 2560: 2.0436081384473472\n",
      "Cost after epoch 2570: 2.0435745385857818\n",
      "Cost after epoch 2580: 2.0435414997138395\n",
      "Cost after epoch 2590: 2.0435089873483028\n",
      "Cost after epoch 2600: 2.043476964130226\n",
      "Cost after epoch 2610: 2.0434459956771893\n",
      "Cost after epoch 2620: 2.043415486491186\n",
      "Cost after epoch 2630: 2.043385322549087\n",
      "Cost after epoch 2640: 2.043355456335828\n",
      "Cost after epoch 2650: 2.043325838398369\n",
      "Cost after epoch 2660: 2.043296985093368\n",
      "Cost after epoch 2670: 2.0432683346774496\n",
      "Cost after epoch 2680: 2.0432397694295585\n",
      "Cost after epoch 2690: 2.043211234294266\n",
      "Cost after epoch 2700: 2.0431826732214637\n",
      "Cost after epoch 2710: 2.0431545838603\n",
      "Cost after epoch 2720: 2.04312642212651\n",
      "Cost after epoch 2730: 2.0430980689486375\n",
      "Cost after epoch 2740: 2.04306946675364\n",
      "Cost after epoch 2750: 2.0430405578397144\n",
      "Cost after epoch 2760: 2.043011851389561\n",
      "Cost after epoch 2770: 2.04298280295661\n",
      "Cost after epoch 2780: 2.0429532936541563\n",
      "Cost after epoch 2790: 2.042923267630339\n",
      "Cost after epoch 2800: 2.0428926696397585\n",
      "Cost after epoch 2810: 2.042862048045729\n",
      "Cost after epoch 2820: 2.042830839862869\n",
      "Cost after epoch 2830: 2.042798927160169\n",
      "Cost after epoch 2840: 2.0427662591010005\n",
      "Cost after epoch 2850: 2.042732786032116\n",
      "Cost after epoch 2860: 2.04269911893556\n",
      "Cost after epoch 2870: 2.0426646594206046\n",
      "Cost after epoch 2880: 2.042629290639012\n",
      "Cost after epoch 2890: 2.0425929690821514\n",
      "Cost after epoch 2900: 2.0425556528365307\n",
      "Cost after epoch 2910: 2.0425180337543014\n",
      "Cost after epoch 2920: 2.042479463584481\n",
      "Cost after epoch 2930: 2.042439826204106\n",
      "Cost after epoch 2940: 2.042399086819487\n",
      "Cost after epoch 2950: 2.042357212487168\n",
      "Cost after epoch 2960: 2.0423149885774765\n",
      "Cost after epoch 2970: 2.0422717056638486\n",
      "Cost after epoch 2980: 2.0422272476774546\n",
      "Cost after epoch 2990: 2.042181589116962\n",
      "Cost after epoch 3000: 2.042134706448383\n",
      "Cost after epoch 3010: 2.0420874856470332\n",
      "Cost after epoch 3020: 2.0420391483138673\n",
      "Cost after epoch 3030: 2.0419895776113557\n",
      "Cost after epoch 3040: 2.0419387572480057\n",
      "Cost after epoch 3050: 2.0418866729031095\n",
      "Cost after epoch 3060: 2.0418343130707144\n",
      "Cost after epoch 3070: 2.0417808261781687\n",
      "Cost after epoch 3080: 2.041726093783849\n",
      "Cost after epoch 3090: 2.0416701082242685\n",
      "Cost after epoch 3100: 2.0416128637227433\n",
      "Cost after epoch 3110: 2.0415554486573946\n",
      "Cost after epoch 3120: 2.0414969375124996\n",
      "Cost after epoch 3130: 2.0414372095031643\n",
      "Cost after epoch 3140: 2.0413762646678215\n",
      "Cost after epoch 3150: 2.0413141047857795\n",
      "Cost after epoch 3160: 2.04125191169899\n",
      "Cost after epoch 3170: 2.041188688269245\n",
      "Cost after epoch 3180: 2.0411243108013384\n",
      "Cost after epoch 3190: 2.041058785903819\n",
      "Cost after epoch 3200: 2.040992121741142\n",
      "Cost after epoch 3210: 2.040925584232033\n",
      "Cost after epoch 3220: 2.040858109568736\n",
      "Cost after epoch 3230: 2.040789570795258\n",
      "Cost after epoch 3240: 2.0407199798685633\n",
      "Cost after epoch 3250: 2.0406493500967944\n",
      "Cost after epoch 3260: 2.0405790199323275\n",
      "Cost after epoch 3270: 2.040507865963835\n",
      "Cost after epoch 3280: 2.0404357578574848\n",
      "Cost after epoch 3290: 2.040362711697819\n",
      "Cost after epoch 3300: 2.0402887447113653\n",
      "Cost after epoch 3310: 2.040215254941494\n",
      "Cost after epoch 3320: 2.040141068016563\n",
      "Cost after epoch 3330: 2.0400660503358785\n",
      "Cost after epoch 3340: 2.0399902209578116\n",
      "Cost after epoch 3350: 2.0399135998804327\n",
      "Cost after epoch 3360: 2.0398376310682242\n",
      "Cost after epoch 3370: 2.0397610988924764\n",
      "Cost after epoch 3380: 2.03968386680282\n",
      "Cost after epoch 3390: 2.0396059557888724\n",
      "Cost after epoch 3400: 2.0395273875923756\n",
      "Cost after epoch 3410: 2.0394496382735046\n",
      "Cost after epoch 3420: 2.039371461235776\n",
      "Cost after epoch 3430: 2.0392927174699254\n",
      "Cost after epoch 3440: 2.039213428990756\n",
      "Cost after epoch 3450: 2.039133618397106\n",
      "Cost after epoch 3460: 2.039054780329309\n",
      "Cost after epoch 3470: 2.038975647683641\n",
      "Cost after epoch 3480: 2.0388960796179227\n",
      "Cost after epoch 3490: 2.0388160984130574\n",
      "Cost after epoch 3500: 2.0387357267877406\n",
      "Cost after epoch 3510: 2.0386564651182133\n",
      "Cost after epoch 3520: 2.0385770360778235\n",
      "Cost after epoch 3530: 2.038497297712535\n",
      "Cost after epoch 3540: 2.0384172719565683\n",
      "Cost after epoch 3550: 2.0383369810580536\n",
      "Cost after epoch 3560: 2.038257919221226\n",
      "Cost after epoch 3570: 2.038178808726565\n",
      "Cost after epoch 3580: 2.0380995072856027\n",
      "Cost after epoch 3590: 2.0380200360109613\n",
      "Cost after epoch 3600: 2.0379404162268733\n",
      "Cost after epoch 3610: 2.037862125173581\n",
      "Cost after epoch 3620: 2.037783893903533\n",
      "Cost after epoch 3630: 2.0377055805924624\n",
      "Cost after epoch 3640: 2.0376272051810322\n",
      "Cost after epoch 3650: 2.0375487877391554\n",
      "Cost after epoch 3660: 2.037471778973634\n",
      "Cost after epoch 3670: 2.0373949270591596\n",
      "Cost after epoch 3680: 2.0373180914252287\n",
      "Cost after epoch 3690: 2.037241290598798\n",
      "Cost after epoch 3700: 2.0371645431715266\n",
      "Cost after epoch 3710: 2.0370892650163044\n",
      "Cost after epoch 3720: 2.037014228870775\n",
      "Cost after epoch 3730: 2.0369392961739186\n",
      "Cost after epoch 3740: 2.0368644838890195\n",
      "Cost after epoch 3750: 2.0367898089949708\n",
      "Cost after epoch 3760: 2.036716645497821\n",
      "Cost after epoch 3770: 2.0366437971804916\n",
      "Cost after epoch 3780: 2.0365711281941405\n",
      "Cost after epoch 3790: 2.0364986538636773\n",
      "Cost after epoch 3800: 2.0364263894936103\n",
      "Cost after epoch 3810: 2.0363556614196576\n",
      "Cost after epoch 3820: 2.0362853099829836\n",
      "Cost after epoch 3830: 2.036215202682762\n",
      "Cost after epoch 3840: 2.036145353190682\n",
      "Cost after epoch 3850: 2.0360757751328475\n",
      "Cost after epoch 3860: 2.036007742535705\n",
      "Cost after epoch 3870: 2.0359401368612255\n",
      "Cost after epoch 3880: 2.03587282951652\n",
      "Cost after epoch 3890: 2.035805832551259\n",
      "Cost after epoch 3900: 2.0357391579530937\n",
      "Cost after epoch 3910: 2.0356740238815547\n",
      "Cost after epoch 3920: 2.0356093565731253\n",
      "Cost after epoch 3930: 2.0355450318219885\n",
      "Cost after epoch 3940: 2.0354810601210342\n",
      "Cost after epoch 3950: 2.0354174518916075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 3960: 2.0353553668552733\n",
      "Cost after epoch 3970: 2.0352937788256775\n",
      "Cost after epoch 3980: 2.0352325683809798\n",
      "Cost after epoch 3990: 2.0351717445464055\n",
      "Cost after epoch 4000: 2.0351113162714416\n",
      "Cost after epoch 4010: 2.0350523831546736\n",
      "Cost after epoch 4020: 2.034993968607089\n",
      "Cost after epoch 4030: 2.0349359583067996\n",
      "Cost after epoch 4040: 2.034878359916002\n",
      "Cost after epoch 4050: 2.034821181020933\n",
      "Cost after epoch 4060: 2.034765460196202\n",
      "Cost after epoch 4070: 2.034710271762666\n",
      "Cost after epoch 4080: 2.0346555067368723\n",
      "Cost after epoch 4090: 2.034601171531386\n",
      "Cost after epoch 4100: 2.034547272485476\n",
      "Cost after epoch 4110: 2.0344947869325503\n",
      "Cost after epoch 4120: 2.0344428407857262\n",
      "Cost after epoch 4130: 2.034391330565884\n",
      "Cost after epoch 4140: 2.0343402615528587\n",
      "Cost after epoch 4150: 2.0342896389578202\n",
      "Cost after epoch 4160: 2.0342403792435095\n",
      "Cost after epoch 4170: 2.034191660044852\n",
      "Cost after epoch 4180: 2.034143383486355\n",
      "Cost after epoch 4190: 2.03409555383149\n",
      "Cost after epoch 4200: 2.0340481752809505\n",
      "Cost after epoch 4210: 2.0340021042949163\n",
      "Cost after epoch 4220: 2.033956569881876\n",
      "Cost after epoch 4230: 2.033911479809204\n",
      "Cost after epoch 4240: 2.033866837436878\n",
      "Cost after epoch 4250: 2.0338226460686903\n",
      "Cost after epoch 4260: 2.0337797034446576\n",
      "Cost after epoch 4270: 2.0337372891920116\n",
      "Cost after epoch 4280: 2.0336953167070257\n",
      "Cost after epoch 4290: 2.03365378855347\n",
      "Cost after epoch 4300: 2.033612707245804\n",
      "Cost after epoch 4310: 2.0335728134247653\n",
      "Cost after epoch 4320: 2.033533436241327\n",
      "Cost after epoch 4330: 2.033494494660328\n",
      "Cost after epoch 4340: 2.0334559905496623\n",
      "Cost after epoch 4350: 2.0334179257347533\n",
      "Cost after epoch 4360: 2.0333809856482823\n",
      "Cost after epoch 4370: 2.033344547590729\n",
      "Cost after epoch 4380: 2.033308535995463\n",
      "Cost after epoch 4390: 2.033272952127061\n",
      "Cost after epoch 4400: 2.0332377972142117\n",
      "Cost after epoch 4410: 2.0332037035825716\n",
      "Cost after epoch 4420: 2.033170095083831\n",
      "Cost after epoch 4430: 2.0331369014864933\n",
      "Cost after epoch 4440: 2.033104123536223\n",
      "Cost after epoch 4450: 2.033071761948985\n",
      "Cost after epoch 4460: 2.0330403982010385\n",
      "Cost after epoch 4470: 2.03300950092229\n",
      "Cost after epoch 4480: 2.032979005055518\n",
      "Cost after epoch 4490: 2.0329489109036962\n",
      "Cost after epoch 4500: 2.0329192187457825\n",
      "Cost after epoch 4510: 2.032890461576601\n",
      "Cost after epoch 4520: 2.0328621509002165\n",
      "Cost after epoch 4530: 2.032834226650795\n",
      "Cost after epoch 4540: 2.0328066887568457\n",
      "Cost after epoch 4550: 2.0327795371280004\n",
      "Cost after epoch 4560: 2.0327532587160864\n",
      "Cost after epoch 4570: 2.032727405902395\n",
      "Cost after epoch 4580: 2.0327019234124752\n",
      "Cost after epoch 4590: 2.032676810860976\n",
      "Cost after epoch 4600: 2.032652067848247\n",
      "Cost after epoch 4610: 2.0326281377580746\n",
      "Cost after epoch 4620: 2.03260461179224\n",
      "Cost after epoch 4630: 2.0325814392549337\n",
      "Cost after epoch 4640: 2.032558619500411\n",
      "Cost after epoch 4650: 2.0325361518726535\n",
      "Cost after epoch 4660: 2.032514438670248\n",
      "Cost after epoch 4670: 2.0324931078272397\n",
      "Cost after epoch 4680: 2.0324721130048653\n",
      "Cost after epoch 4690: 2.0324514533438673\n",
      "Cost after epoch 4700: 2.0324311279782146\n",
      "Cost after epoch 4710: 2.032411500588134\n",
      "Cost after epoch 4720: 2.032392233744237\n",
      "Cost after epoch 4730: 2.032373285237729\n",
      "Cost after epoch 4740: 2.0323546540367277\n",
      "Cost after epoch 4750: 2.032336339105584\n",
      "Cost after epoch 4760: 2.032318667937314\n",
      "Cost after epoch 4770: 2.0323013356560677\n",
      "Cost after epoch 4780: 2.03228430395338\n",
      "Cost after epoch 4790: 2.0322675716602183\n",
      "Cost after epoch 4800: 2.0322511376063432\n",
      "Cost after epoch 4810: 2.032235295477059\n",
      "Cost after epoch 4820: 2.0322197708963423\n",
      "Cost after epoch 4830: 2.0322045292263415\n",
      "Cost after epoch 4840: 2.0321895691915213\n",
      "Cost after epoch 4850: 2.032174889517302\n",
      "Cost after epoch 4860: 2.032160752396326\n",
      "Cost after epoch 4870: 2.0321469119416267\n",
      "Cost after epoch 4880: 2.032133336958267\n",
      "Cost after epoch 4890: 2.0321200260905763\n",
      "Cost after epoch 4900: 2.032106977985644\n",
      "Cost after epoch 4910: 2.032094425584076\n",
      "Cost after epoch 4920: 2.0320821495310444\n",
      "Cost after epoch 4930: 2.032070121850593\n",
      "Cost after epoch 4940: 2.0320583411295132\n",
      "Cost after epoch 4950: 2.0320468059588492\n",
      "Cost after epoch 4960: 2.032035722185729\n",
      "Cost after epoch 4970: 2.0320248950930275\n",
      "Cost after epoch 4980: 2.032014299705002\n",
      "Cost after epoch 4990: 2.032003934570206\n",
      "training time: 0:00:07.695609\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "paras = skipgram_model_training(X, Y_one_hot, vocab_size, 50, 0.05, 5000, batch_size=128, parameters=None, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.arange(vocab_size)\n",
    "X_test = np.expand_dims(X_test, axis=0)\n",
    "softmax_test, _ = forward_propagation(X_test, paras)\n",
    "top_sorted_inds = np.argsort(softmax_test, axis=0)[-4:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a's neighbor words: ['game', 'market', \"loser's\", 'stock']\n",
      "of's neighbor words: ['the', 'of', 'costs', 'beating']\n",
      "deduction's neighbor words: ['costs', 'after', 'the', 'beating']\n",
      "beating's neighbor words: ['market', 'stock', 'costs', 'investing']\n",
      "game's neighbor words: ['is', \"loser's\", 'a', 'beating']\n",
      "the's neighbor words: ['is', 'of', 'beating', 'stock']\n",
      "is's neighbor words: ['game', 'market', \"loser's\", 'a']\n",
      "stock's neighbor words: ['investing', 'a', 'is', 'market']\n",
      "loser's's neighbor words: ['game', 'market', \"loser's\", 'stock']\n",
      "investing's neighbor words: ['the', 'stock', 'beating', \"loser's\"]\n",
      "costs's neighbor words: ['beating', 'of', 'deduction', 'the']\n",
      "market's neighbor words: ['is', \"loser's\", 'a', 'beating']\n",
      "after's neighbor words: ['of', 'the', 'deduction', 'costs']\n"
     ]
    }
   ],
   "source": [
    "for input_ind in range(vocab_size):\n",
    "    input_word = id_to_word[input_ind]\n",
    "    output_words = [id_to_word[output_ind] for output_ind in top_sorted_inds[::-1, input_ind]]\n",
    "    print(\"{}'s neighbor words: {}\".format(input_word, output_words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
